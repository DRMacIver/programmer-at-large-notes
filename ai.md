# Artificial Intelligence

The following are the rules for AI in this story (I consider them to be plausible
rules for how things would work in reality but not necessarily true. For the
sake of the story they are true):

* Sentient AI is possible, and can be made at or slightly above the upper end of human intelligence.
* Godlike superintelligent AI is not possible. There are fundamental scaling limits
  for intelligence.
* Copying an AI with fidelity is hard to impossible - they tend to end up depending quite
  sensitively on specific quirks of their architecture and putting them on a new architecture
  tends to cause them to rapidly diverge.
* AIs run very badly on architectures optimised for general purpose computing - it's not
  *impossible* to make them run on an arbitrary machine, but it tends to require at least
  and order of magnitude more resources than it should and they don't run at full speed.
* General intelligence tends to be produce non-copyable black boxes. You can't just take
  knowledge from one AI and stick it into another AI.
* As a result of all of the above it is important to note that AI are physical objects. An
  AI is just as bound to its body as a human is, that body is just smaller and contains a
  larger quantity of metal and silicon.
  
## Resource Constraints on AI

* AI are not actually especially cheap to make compared to humans. You can't just make
  an AI out of whole cloth - they need to be grown from a seed. That seed is a more
  reliable starting point than a human infant, but it's not a *much* more reliable starting
  point.
* There tends to be a fast/stable tradeoff when growing an AI. It's not that you can't get a
  useful AI in under a year, it's just that you'll have to throw away 90% of them at the end
  of that because they're unproductively insane (cultural viewpoints on whether this counts
  as murder vary, but it is murder)
* The lack of self-repairing machines means that an AI is dependent on a much larger industrial
  base than humans are. If an AI runs into problems then the parts to fix it had better be
  available or easy to manufacture.

## What happens when you make an AI?

* There is no singularity per se, certainly no hard take off one.
* But it tends not to go very well for the society in question due to complex and
  not very well understood social dynamics. Sentient AIs tend to be a harbringer of
  the end times, and societies where they are widespread tend to destabilise and
  collapse within a generation or two.
* The AI and automation rarely lasts much longer than a few centuries after the
  collapse of the civilization that built it: The relative cost and complexity of
  maintaining an AI means that there are enough parts in the pipeline for producing
  it that it really needs a large industrial base with a sentient workforce to keep
  an AI running indefinitely. The numbers mostly end up not working out for AI
  only civilization, and the hard crashes that tend to come for human civilizations
  with AI in them tend not to lead to an infrastructure that can support them afterwards.

## Trade Fleet Attitudes to AI

Official trade fleet policy is that when you encounter a sentient AI you adopt
the following procedures:

1. Immediately shut down, quarantine and flag any software you have bought from
   this system. Dump all physical goods where possible. If the local system has
   made modifications to your ship, cut as many of them out with a hacksaw as you
   can without interfering with step 2.
2. Get all your crew back on ship as quickly as possible and then bug out of the
   system at maximum acceleration, pausing only to leave a monitoring and warning
   buoy broadcasting outsystem for other trade fleet vessels to hear.

This is arguably more paranoid than is strictly necessary - fears of super hacking
AI are mostly unfounded - but ship lineages that implement policies with less fear
of AI have a tendency not to outlive their first 500 years or so, which means
this policy is as close to universal within trade fleet as you get. 

## Why haven't AI taken over the galaxy?

The general reason for this in setting is that organic intelligences outcompete
AI in the long run due to robustness advantages and a greater desire to propagate.

In particular:

* The competitive advantage of an individual AI against an individual human is
  just not that much greater than you'd expect: They're often smarter and can
  almost always process data more quickly, but the human is just as capable of
  getting support from an extensive network of automation and software as the AI is,
  and the AI can't just download kung fu into their brain any more than the
  human can.
* AI civilizations are just as vulnerable to periodic collapse as organic ones
  but when an AI civilization collapses everyone dies (eventually) because they
  can no longer maintain themselves.
* AI civilizations need to be larger than organic ones in order to be viable,
  because of the greater required industrial base, which means that they are
  less prone to colonization (on the plus side they don't require terraforming
  first, but the terraforming problem has largely been licked and isn't the
  bottleneck for human colonisation).
* Generally speaking, AI don't want to. The desire to reproduce and expand is
  more acutely felt by organic intelligences.
* The trade fleet acts as a major competitive advantage for organic intelligence
  because they are very good at propagating knowledge across the broader sphere
  of humanity, and their complete unwillingness to deal with AI means that a
  willingness to build AI is strongly selected against in humanity's cultural
  evolution.
* Invading an established system is so hard as to not be worth bothering with,
  so even were AI to "take over the galaxy" it would largely look like a parallel
  civilization that mostly occupies solar systems that humans can't be bothered
  with.

So what happens instead is that AI based civilizations are rare, tend not to
spread themselves, and where they spread themselves tend to do so to places
that humanity doesn't care about.
